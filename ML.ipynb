{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9d7f65",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [ML notebook](#toc1_)    \n",
    "  - [Assemble dataset](#toc1_1_)    \n",
    "  - [Prepare the dataset](#toc1_2_)    \n",
    "- [Unet](#toc2_)    \n",
    "  - [Save the weights](#toc2_1_)    \n",
    "- [Training results](#toc3_)    \n",
    "- [Inference](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[ML notebook](#toc0_)\n",
    "17.02.2025 -> 04.06.2025 - Dominique Humbert\n",
    "Initial version.\n",
    "\n",
    "Usefull links with which the Implementation was built:\n",
    "- https://medium.com/coinmonks/learn-how-to-train-u-net-on-your-dataset-8e3f89fbd623\n",
    "\n",
    "Model U-net from:\n",
    "Vanberg, P.-O. (2019). Machine learning for image-based wavefront sensing. Université de Liège, Liège, Belgique.\n",
    "https://explore.lib.uliege.be/permalink/32ULG_INST/oao96e/alma9919993582302321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from ows import ows\n",
    "from ows import fouriertransform as mathft\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import SqrtStretch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff56855",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 25\n",
    "GPU = True\n",
    "foldername = \"unet_outputs/Dw_N16_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Assemble dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 99999\n",
    "dim_data = 64\n",
    "# train_DwDx = np.zeros((n,128,128))\n",
    "# train_DwDy = np.zeros((n,128,128))\n",
    "train_Dw = np.zeros((n,dim_data,dim_data,2))\n",
    "#train_Dw = np.zeros((n,128,128))\n",
    "train_lightfield = np.zeros((n,dim_data,dim_data))\n",
    "train_psf = np.zeros((n,dim_data,dim_data))\n",
    "train_ps = np.zeros((n,dim_data,dim_data))\n",
    "print(train_Dw.shape)\n",
    "filename0 = \"data/DwDx_\"\n",
    "filename1 = \"data/DwDy_\"\n",
    "filename2 = \"data/lightfield_\"\n",
    "filename3 = \"data/psf_\"\n",
    "filename4 = \"data/ps_\"\n",
    "\n",
    "status = 0\n",
    "for i in range(0,n):\n",
    "    train_Dw[i,:,:,0] = ows.pixel_adder(np.array(fits.getdata(filename0+str(i)+\".fits\")), scale_factor = [4,4], final_shape = None)\n",
    "    # train_Dw[i,:,:,0] = np.array(fits.getdata(filename0+str(i)+\".fits\"))\n",
    "    train_Dw[i,:,:,1] = ows.pixel_adder(np.array(fits.getdata(filename1+str(i)+\".fits\")), scale_factor = [4,4], final_shape = None)\n",
    "    # train_Dw[i,:,:,1] = np.array(fits.getdata(filename1+str(i)+\".fits\"))\n",
    "\n",
    "    # train_lightfield[i,:,:] = ows.pixel_adder(np.array(fits.getdata(filename2+str(i)+\".fits\")),[1/4,1/4])\n",
    "    train_lightfield[i,:,:] = np.array(fits.getdata(filename2+str(i)+\".fits\"))\n",
    "    # train_psf[i,:,:] = ows.pixel_adder(np.array(fits.getdata(filename3+str(i)+\".fits\")),[1/4,1/4])\n",
    "    train_psf[i,:,:] = (np.array(fits.getdata(filename3+str(i)+\".fits\")))\n",
    "    # train_ps[i,:,:] = ows.normalize(np.array(fits.getdata(filename4+str(i)+\".fits\")))\n",
    "    if 100*i//n == status*10:\n",
    "        print('Loading training set: ',status*10,'% done')\n",
    "        status += 1\n",
    "print('Loading training set: ',100,'% done')\n",
    "#np.savez(\"data/train_DwDx.npz\", train_DwDy)\n",
    "#np.savez(\"data/train_DwDy.npz\", train_DwDy)\n",
    "np.savez(\"data/train_Dw.npz\", train_Dw)\n",
    "np.savez(\"data/train_lightfield.npz\", train_lightfield)\n",
    "np.savez(\"data/train_psf.npz\", train_psf)\n",
    "# np.savez(\"data/train_ps.npz\", train_ps)\n",
    "\n",
    "plt.close()\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(train_psf[0,:,:])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(train_lightfield[0,:,:])\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(train_Dw[0,:,:,0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(train_Dw[0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Prepare the dataset](#toc0_)\n",
    "\n",
    "1. Random draw to separate the training and validation set\n",
    "2. Split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=None)\n",
    "n = 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21678ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "rng = np.random.default_rng(n)\n",
    "temp = np.load(\"data/train_Dw.npz\")[\"arr_0\"]\n",
    "n = temp.shape[0]\n",
    "k = 0.8 \n",
    "train_id = np.zeros(n,np.bool)\n",
    "\n",
    "train_id[rng.choice(n,int(k*n),replace=False)] = True\n",
    "\n",
    "norm_max = np.max(temp)\n",
    "norm_min = np.min(temp)\n",
    "\n",
    "temp = (temp - norm_min) / (norm_max - norm_min)\n",
    "\n",
    "temp = np.array(temp)\n",
    "print(temp.shape)\n",
    "x_train = temp[train_id,:,:]\n",
    "x_val = temp[~train_id,:,:]\n",
    "# y_train = temp[train_id,:,:]\n",
    "# y_val = temp[~train_id,:,:]\n",
    "# print(x_train.shape)\n",
    "\n",
    "temp = np.load(\"data/train_psf.npz\")[\"arr_0\"]\n",
    "\n",
    "y_train = temp[train_id,:,:]\n",
    "y_val = temp[~train_id,:,:]\n",
    "# x_train = temp[train_id,:,:]\n",
    "# x_val = temp[~train_id,:,:]\n",
    "\n",
    "plt.close(0)\n",
    "plt.figure(0)\n",
    "plt.imshow(x_train[4,:,:,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(x_val.shape,x_train.shape)\n",
    "print(y_val.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Unet](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9669391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ows.unet as unet\n",
    "model = unet.build_unet(input_shape=(64,64,1), n_channels_out=1)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51578db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau \n",
    "weight_path=\"{}_weights.best.weights.h5\".format('cxr_reg') \n",
    " \n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,  \n",
    "                             save_best_only=True, mode='min', save_weights_only = True) \n",
    " \n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, \n",
    "                                   verbose=1, mode='min', epsilon=0.05, cooldown=2, min_lr=1e-6) \n",
    " \n",
    "early = EarlyStopping(monitor=\"val_loss\",  mode=\"min\", patience=15)  \n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "from IPython.display import clear_output \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import roc_curve, auc \n",
    "#images, mask = images/255, (mask>127).astype(np.float32) \n",
    "                                                            \n",
    "train_vol = x_train\n",
    "\n",
    "validation_vol = x_val\n",
    "train_seg = y_train\n",
    "validation_seg = y_val\n",
    "\n",
    "print(train_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU:\n",
    "    import tensorflow as tf\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    with tf.device('/GPU:0'):\n",
    "    model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9), loss=[unet.mse_loss], metrics = [unet.dice_coef, 'binary_accuracy',\"AUC\" ]) \n",
    "\n",
    "    loss_history = model.fit(x = train_vol,y = train_seg,batch_size = 32,epochs = 100,validation_data =(validation_vol,validation_seg) , callbacks=callbacks_list)\n",
    "else:\n",
    "    loss_history = model.fit(x = train_vol,y = train_seg,batch_size = 12,epochs = 100,validation_data =(validation_vol,validation_seg) , callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Save the weights](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(foldername + \"/model.weights.h5\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Training results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "ax1.plot(loss_history.history['loss'], '-', label = 'Loss')\n",
    "ax1.plot(loss_history.history['val_loss'], '-', label = 'Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss History')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(100*np.array(loss_history.history['binary_accuracy']), '-', label = 'Accuracy')\n",
    "ax2.plot(100*np.array(loss_history.history['val_binary_accuracy']), '-',label = 'Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Accuracy History')\n",
    "ax2.legend()\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.savefig(foldername + \"/loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(foldername + \"/model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DwDx = ows.pixel_adder(np.array(fits.getdata(\"data/DwDx_99999.fits\")), scale_factor = [4,4], final_shape = None)\n",
    "DwDy = ows.pixel_adder(np.array(fits.getdata(\"data/DwDy_99999.fits\")), scale_factor = [4,4], final_shape = None)\n",
    "# DwDx = np.array(fits.getdata(\"data/DwDx_99999.fits\"))\n",
    "# DwDy = np.array(fits.getdata(\"data/DwDy_99999.fits\"))\n",
    "# psf = ows.pixel_adder(np.array(fits.getdata(\"data/psf_99999.fits\")), scale_factor = [1/4,1/4], final_shape = None)\n",
    "psf = np.array(fits.getdata(\"data/psf_99999.fits\"))\n",
    "# ps = np.array(fits.getdata(\"data/ps_99999.fits\"))\n",
    "lightfield = np.array(fits.getdata(\"data/lightfield_99999.fits\"))\n",
    "\n",
    "\n",
    "\n",
    "Dw = np.zeros((64,64,2))\n",
    "print(DwDx.shape)\n",
    "Dw[:,:,0] = DwDx\n",
    "Dw[:,:,1] = DwDy\n",
    "\n",
    "Dw = (Dw - norm_min) / (norm_max - norm_min)\n",
    "\n",
    "im = np.expand_dims(Dw, axis=0)\n",
    "\n",
    "predictions = model(im)[0,:,:,0]\n",
    "# print(predictions[:,:,0].shape)\n",
    "\n",
    "\n",
    "plt.close(2)\n",
    "plt.figure(2,figsize = (10, 10))\n",
    "plt.imshow(predictions)\n",
    "plt.title(\"Inference\")\n",
    "plt.colorbar()\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.savefig(foldername + \"/inference.png\")\n",
    "fits.writeto(foldername+\"/inference.fits\", np.asarray(predictions), overwrite=True)\n",
    "\n",
    "plt.close(3)\n",
    "plt.figure(3,figsize = (10, 10))\n",
    "plt.imshow(psf[:,:])\n",
    "plt.title(\"True image\")\n",
    "plt.colorbar()\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.savefig(foldername + \"/true_image.png\")\n",
    "fits.writeto(foldername+\"/true_image.fits\", psf[:,:], overwrite=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
